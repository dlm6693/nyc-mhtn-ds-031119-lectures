{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# import xgboost as XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **learning_rate**: step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "- **max_depth**: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "- **subsample**: percentage of samples used per tree. Low value can lead to underfitting.\n",
    "- **colsample_bytree**: percentage of features used per tree. High value can lead to overfitting.\n",
    "- **n_estimators**: number of trees you want to build.\n",
    "- **objective**: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n",
    "\n",
    "- **gamma**: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n",
    "- **alpha**: L1 regularization on leaf weights. A large value leads to more regularization.\n",
    "- **lambda**: L2 regularization on leaf weights and is smoother than L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 2, alpha = 10, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.711409\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\xgboost-0.83.dev0-py3.7.egg\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\xgboost-0.83.dev0-py3.7.egg\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.663809</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.664432</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649386</td>\n",
       "      <td>0.007769</td>\n",
       "      <td>0.650293</td>\n",
       "      <td>0.009091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639790</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.641551</td>\n",
       "      <td>0.009498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.632822</td>\n",
       "      <td>0.010948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.627469</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>0.011235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.614946</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.617066</td>\n",
       "      <td>0.013344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.612587</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.614983</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.609946</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.612319</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.603315</td>\n",
       "      <td>0.010589</td>\n",
       "      <td>0.606532</td>\n",
       "      <td>0.012787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.597327</td>\n",
       "      <td>0.012510</td>\n",
       "      <td>0.601291</td>\n",
       "      <td>0.015202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.588167</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.592322</td>\n",
       "      <td>0.014094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.581364</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.586414</td>\n",
       "      <td>0.015523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.575114</td>\n",
       "      <td>0.013959</td>\n",
       "      <td>0.580137</td>\n",
       "      <td>0.014922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.572616</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.576990</td>\n",
       "      <td>0.014829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.568252</td>\n",
       "      <td>0.014617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.561297</td>\n",
       "      <td>0.013019</td>\n",
       "      <td>0.565745</td>\n",
       "      <td>0.015584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.558712</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.563686</td>\n",
       "      <td>0.015801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.554567</td>\n",
       "      <td>0.014198</td>\n",
       "      <td>0.560045</td>\n",
       "      <td>0.016332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.550429</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.556494</td>\n",
       "      <td>0.016353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.543221</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.548416</td>\n",
       "      <td>0.018552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.541424</td>\n",
       "      <td>0.012434</td>\n",
       "      <td>0.546424</td>\n",
       "      <td>0.018445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.536245</td>\n",
       "      <td>0.011849</td>\n",
       "      <td>0.541606</td>\n",
       "      <td>0.021241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.533631</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.539002</td>\n",
       "      <td>0.022125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.534087</td>\n",
       "      <td>0.018871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.527310</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>0.533431</td>\n",
       "      <td>0.018430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.523743</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.530258</td>\n",
       "      <td>0.019434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.520959</td>\n",
       "      <td>0.011221</td>\n",
       "      <td>0.527432</td>\n",
       "      <td>0.020908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.519084</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.525620</td>\n",
       "      <td>0.022354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.517246</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.524137</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.514742</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.521513</td>\n",
       "      <td>0.022360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.464381</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>0.020044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.464091</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.474147</td>\n",
       "      <td>0.020225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.473996</td>\n",
       "      <td>0.020427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.463485</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.473733</td>\n",
       "      <td>0.020174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.463328</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.473635</td>\n",
       "      <td>0.020218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.462006</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.472294</td>\n",
       "      <td>0.020174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.461298</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.471805</td>\n",
       "      <td>0.020167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.461040</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.471471</td>\n",
       "      <td>0.020115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.460987</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.471434</td>\n",
       "      <td>0.020130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.460371</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.470777</td>\n",
       "      <td>0.019771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.460127</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.470594</td>\n",
       "      <td>0.019731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.459780</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.470334</td>\n",
       "      <td>0.019562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.458475</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.469278</td>\n",
       "      <td>0.019631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.458188</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.468919</td>\n",
       "      <td>0.019927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.457841</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.019897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.457417</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.468384</td>\n",
       "      <td>0.019607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.457055</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.468297</td>\n",
       "      <td>0.019599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.456674</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.467915</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.456385</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.467624</td>\n",
       "      <td>0.019307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.455894</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.467211</td>\n",
       "      <td>0.019358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.455346</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>0.466630</td>\n",
       "      <td>0.018741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.455079</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.466419</td>\n",
       "      <td>0.018812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.454612</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.465894</td>\n",
       "      <td>0.019007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.454384</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.465663</td>\n",
       "      <td>0.019017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.454119</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.465465</td>\n",
       "      <td>0.019085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.453907</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.465337</td>\n",
       "      <td>0.019285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.018774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.453027</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.464463</td>\n",
       "      <td>0.018912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.452747</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.464222</td>\n",
       "      <td>0.018858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.452426</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>0.463821</td>\n",
       "      <td>0.018884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-logloss-mean  train-logloss-std  test-logloss-mean  test-logloss-std\n",
       "0             0.663809           0.000768           0.664432          0.001201\n",
       "1             0.649386           0.007769           0.650293          0.009091\n",
       "2             0.639790           0.008282           0.641551          0.009498\n",
       "3             0.630923           0.011054           0.632822          0.010948\n",
       "4             0.627469           0.011194           0.629472          0.011235\n",
       "5             0.614946           0.011402           0.617066          0.013344\n",
       "6             0.612587           0.011340           0.614983          0.013672\n",
       "7             0.609946           0.011559           0.612319          0.013900\n",
       "8             0.603315           0.010589           0.606532          0.012787\n",
       "9             0.597327           0.012510           0.601291          0.015202\n",
       "10            0.588167           0.010663           0.592322          0.014094\n",
       "11            0.581364           0.013091           0.586414          0.015523\n",
       "12            0.575114           0.013959           0.580137          0.014922\n",
       "13            0.572616           0.013546           0.576990          0.014829\n",
       "14            0.564277           0.013384           0.568252          0.014617\n",
       "15            0.561297           0.013019           0.565745          0.015584\n",
       "16            0.558712           0.011906           0.563686          0.015801\n",
       "17            0.554567           0.014198           0.560045          0.016332\n",
       "18            0.550429           0.013912           0.556494          0.016353\n",
       "19            0.543221           0.013333           0.548416          0.018552\n",
       "20            0.541424           0.012434           0.546424          0.018445\n",
       "21            0.536245           0.011849           0.541606          0.021241\n",
       "22            0.533631           0.012959           0.539002          0.022125\n",
       "23            0.528302           0.009912           0.534087          0.018871\n",
       "24            0.527310           0.009666           0.533431          0.018430\n",
       "25            0.523743           0.009430           0.530258          0.019434\n",
       "26            0.520959           0.011221           0.527432          0.020908\n",
       "27            0.519084           0.010605           0.525620          0.022354\n",
       "28            0.517246           0.011812           0.524137          0.022304\n",
       "29            0.514742           0.012771           0.521513          0.022360\n",
       "..                 ...                ...                ...               ...\n",
       "70            0.464381           0.003954           0.474314          0.020044\n",
       "71            0.464091           0.003930           0.474147          0.020225\n",
       "72            0.463941           0.003840           0.473996          0.020427\n",
       "73            0.463485           0.004266           0.473733          0.020174\n",
       "74            0.463328           0.004269           0.473635          0.020218\n",
       "75            0.462006           0.004596           0.472294          0.020174\n",
       "76            0.461298           0.004988           0.471805          0.020167\n",
       "77            0.461040           0.005143           0.471471          0.020115\n",
       "78            0.460987           0.005167           0.471434          0.020130\n",
       "79            0.460371           0.005472           0.470777          0.019771\n",
       "80            0.460127           0.005583           0.470594          0.019731\n",
       "81            0.459780           0.005860           0.470334          0.019562\n",
       "82            0.458475           0.005729           0.469278          0.019631\n",
       "83            0.458188           0.005578           0.468919          0.019927\n",
       "84            0.457841           0.005812           0.468816          0.019897\n",
       "85            0.457417           0.006193           0.468384          0.019607\n",
       "86            0.457055           0.006364           0.468297          0.019599\n",
       "87            0.456674           0.006603           0.467915          0.019498\n",
       "88            0.456385           0.006938           0.467624          0.019307\n",
       "89            0.455894           0.006950           0.467211          0.019358\n",
       "90            0.455346           0.007339           0.466630          0.018741\n",
       "91            0.455079           0.007371           0.466419          0.018812\n",
       "92            0.454612           0.007353           0.465894          0.019007\n",
       "93            0.454384           0.007362           0.465663          0.019017\n",
       "94            0.454119           0.007371           0.465465          0.019085\n",
       "95            0.453907           0.007191           0.465337          0.019285\n",
       "96            0.453300           0.007449           0.464722          0.018774\n",
       "97            0.453027           0.007478           0.464463          0.018912\n",
       "98            0.452747           0.007636           0.464222          0.018858\n",
       "99            0.452426           0.007649           0.463821          0.018884\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 2, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n",
    "                    num_boost_round=100,early_stopping_rounds=5,metrics=\"logloss\", as_pandas=True, seed=123)\n",
    "cv_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    xgboost_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9249\n",
      "AUC Score (Train): 0.977097\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=3,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11450761, 0.04253057, 0.07107796, 0.04770331, 0.04796214,\n",
       "       0.0582455 , 0.5413605 , 0.03617661, 0.04043587], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([0.3087225 , 0.2800467 , 0.26367683, 0.40384126, 0.38894134,\n",
       "         0.36493707, 0.52192445, 0.45475612, 0.40468955, 0.58838339,\n",
       "         0.48034921, 0.37024202]),\n",
       "  'std_fit_time': array([0.01580152, 0.00586575, 0.01542302, 0.01430271, 0.00277315,\n",
       "         0.00914169, 0.00658206, 0.00709667, 0.00386855, 0.0076109 ,\n",
       "         0.01188978, 0.03336647]),\n",
       "  'mean_score_time': array([0.00992365, 0.01329327, 0.0066278 , 0.00735869, 0.00312428,\n",
       "         0.00624919, 0.00342946, 0.00648584, 0.0066062 , 0.00626788,\n",
       "         0.00542722, 0.00533681]),\n",
       "  'std_score_time': array([0.00811456, 0.00668963, 0.00623439, 0.00823025, 0.00624857,\n",
       "         0.00765366, 0.00637017, 0.00777847, 0.0080958 , 0.00767662,\n",
       "         0.00581948, 0.00594705]),\n",
       "  'param_max_depth': masked_array(data=[3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'max_depth': 3, 'min_child_weight': 1},\n",
       "   {'max_depth': 3, 'min_child_weight': 3},\n",
       "   {'max_depth': 3, 'min_child_weight': 5},\n",
       "   {'max_depth': 5, 'min_child_weight': 1},\n",
       "   {'max_depth': 5, 'min_child_weight': 3},\n",
       "   {'max_depth': 5, 'min_child_weight': 5},\n",
       "   {'max_depth': 7, 'min_child_weight': 1},\n",
       "   {'max_depth': 7, 'min_child_weight': 3},\n",
       "   {'max_depth': 7, 'min_child_weight': 5},\n",
       "   {'max_depth': 9, 'min_child_weight': 1},\n",
       "   {'max_depth': 9, 'min_child_weight': 3},\n",
       "   {'max_depth': 9, 'min_child_weight': 5}],\n",
       "  'split0_test_score': array([0.92173305, 0.92313068, 0.92336362, 0.91218262, 0.92592593,\n",
       "         0.91893781, 0.9126485 , 0.92709061, 0.92499418, 0.91031912,\n",
       "         0.92895411, 0.92056837]),\n",
       "  'split1_test_score': array([0.84963895, 0.84381551, 0.84486373, 0.83892383, 0.84859073,\n",
       "         0.84975542, 0.84544608, 0.8490566 , 0.84998835, 0.84381551,\n",
       "         0.85231773, 0.84859073]),\n",
       "  'split2_test_score': array([0.83764267, 0.84043792, 0.83263452, 0.84556254, 0.84556254,\n",
       "         0.83216865, 0.85732588, 0.84556254, 0.83403215, 0.85522944,\n",
       "         0.84742604, 0.83368274]),\n",
       "  'split3_test_score': array([0.89543269, 0.89603365, 0.90036058, 0.88966346, 0.90108173,\n",
       "         0.8984375 , 0.88245192, 0.89915865, 0.9015625 , 0.890625  ,\n",
       "         0.89603365, 0.90204327]),\n",
       "  'split4_test_score': array([0.88004808, 0.88545673, 0.89783654, 0.89110577, 0.90012019,\n",
       "         0.903125  , 0.90096154, 0.89795673, 0.90697115, 0.90192308,\n",
       "         0.89723558, 0.90576923]),\n",
       "  'mean_test_score': array([0.87689909, 0.8777749 , 0.8798118 , 0.87548765, 0.88425622,\n",
       "         0.88048487, 0.87976678, 0.88376503, 0.88350967, 0.88038243,\n",
       "         0.88439342, 0.88213087]),\n",
       "  'std_test_score': array([0.03048828, 0.03161275, 0.03490263, 0.02836838, 0.03175014,\n",
       "         0.03344326, 0.02537377, 0.03155813, 0.03512553, 0.02621041,\n",
       "         0.03059905, 0.03436533]),\n",
       "  'rank_test_score': array([11, 10,  8, 12,  2,  6,  9,  3,  4,  7,  1,  5]),\n",
       "  'split0_train_score': array([0.9599231 , 0.94658385, 0.93493049, 0.98919698, 0.96980183,\n",
       "         0.95170068, 0.99567436, 0.97671547, 0.95371192, 0.99733067,\n",
       "         0.97723307, 0.95292813]),\n",
       "  'split1_train_score': array([0.96679976, 0.95766785, 0.94640639, 0.99021   , 0.97385389,\n",
       "         0.95930938, 0.99547471, 0.97774327, 0.96080302, 0.9972937 ,\n",
       "         0.97924431, 0.96058119]),\n",
       "  'split2_train_score': array([0.96629695, 0.95426649, 0.94687223, 0.98924874, 0.97398699,\n",
       "         0.95744602, 0.99525288, 0.97732919, 0.9599231 , 0.9970423 ,\n",
       "         0.97769891, 0.95950902]),\n",
       "  'split3_train_score': array([0.96493184, 0.94898244, 0.93916629, 0.98854049, 0.96974455,\n",
       "         0.95471953, 0.99426291, 0.97572374, 0.95668569, 0.99564216,\n",
       "         0.97535692, 0.9572506 ]),\n",
       "  'split4_train_score': array([0.96207064, 0.95056711, 0.94072895, 0.98886329, 0.9709037 ,\n",
       "         0.95360439, 0.9950259 , 0.97532757, 0.95590803, 0.99643449,\n",
       "         0.97592182, 0.95573196]),\n",
       "  'mean_train_score': array([0.96400446, 0.95161355, 0.94162087, 0.9892119 , 0.97165819,\n",
       "         0.955356  , 0.99513815, 0.97656785, 0.95740635, 0.99674866,\n",
       "         0.977091  , 0.95720018]),\n",
       "  'std_train_score': array([0.00262017, 0.0039245 , 0.00451795, 0.00056037, 0.00189324,\n",
       "         0.00271411, 0.00048839, 0.0009202 , 0.00261857, 0.00063954,\n",
       "         0.00137117, 0.00272656])},\n",
       " {'max_depth': 9, 'min_child_weight': 3},\n",
       " 0.8843934222078875)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'min_child_weight': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843934222078875"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.671233\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1cb8d9d233b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# plot feature importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_importance' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=nan, n_estimators=140, n_jobs=1,\n",
      "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.8, verbosity=1),\n",
      "       fit_params=None, iid=False, n_jobs=4,\n",
      "       param_grid={'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='roc_auc', verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
       "       min_child_weight=3, missing=nan, n_estimators=140, n_jobs=1,\n",
       "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
